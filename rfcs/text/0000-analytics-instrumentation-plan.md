### DCP PR:

# Analytics instrumentation plan

## Summary

To support collection of the Human Cell Altas (HCA) Data Coordination Platform (DCP) user metrics desired by the DCP
development team, the HCA DCP service must be instrumented with a user tracking solution. This document describes the
goals for the tracking solution and a proposal for instrumenting HCA DCP service components.

## Author(s)

* [Matt Weiden](mailto:mweiden@chanzuckerberg.com)
* [Marcio Von Muhlen](mailto:mvonmuhlen@chanzuckerberg.com)
* [Adrienne Sussman](mailto:%20asussman@chanzuckerberg.com)

## Shepherd

[Trevor Heathorn](mailto:theathor@ucsc.edu)

## Motivation

The development team needs quantitative data on application usage to drive informed decisions on how HCA DCP should
evolve as a product. The following goals address this need.

* Respect user privacy: do not gather personally identifiable information nor build user profiles
* Gather anonymized user browsing activity in the data portal and data browser
* Gather anonymized data download events initiated from any user interface (web, CLI, API)
* Gather anonymized data upload events
* Enable restricted access control to the user metrics data set
* Enable non-developers to access and analyze the resulting data set to improve the HCA DCP inform the HCA scientific
  leadership team

### User Stories or Use Cases

* As an [HCA program, product or project manager](https://github.com/HumanCellAtlas/dcp-community/blob/master/charters/PM/charter.md)
  (PM), I want to analyze quantitative metrics of user actions on DCP, to make informed decisions on product direction.
* As an HCA UX researcher, I want to analyze quantitative metrics of user actions on DCP, to draw insights on our users'
  needs and experiences using the DCP.

## Detailed Design

We’ve selected Google Analytics (GA) as our tracking solution. We briefly considered alternatives, like building our own
and other commercial products. We chose Google Analytics for the following reasons:

* **Community precedent** - Multiple comparable scientific data platforms like ENCODE presently use Google Analytics.
  This is easy to verify by visiting their landing pages and checking for the Google Analytics javascript tag.
* **Usability** - Google Analytics is widely used by non-developers, allowing point-and-click access.
* **Cost** - Google Analytics is free for the user activity levels we anticipate (<10M events per month).
* **Privacy policy** - Google Analytics has a strong privacy policy that prevents Google from accessing and/or
  aggregating user activity data.
* **Access controls** - Google Analytics has a robust access control system that makes it easy to extend and manage
  access. This includes transferring ownership of a tracking account to a different admin.

Downsides also exist. A primary one is that GA is built primarily for traditional web publishers and e-commerce. HCA DCP
does not fit this mold. Thus many core GA features will be distractions, and features like data schema flexibility will
be lacking. These downsides were not significant enough to warrant not using GA.

### Configuration

1. We configure a [tracking ID](https://support.google.com/analytics/answer/1008080?hl=en) in the GA console for each of
   the deployment environments
    1. `dev` - for local development and tests on the dev deployment
    2. `integration` - for integration tests
    3. `staging` - for the staging environment
    4. `prod` - for the production deployment and real usage in the browser

Dividing up metrics collection by deployment environment allows us to do the following. * test that metrics are flowing
for earlier deployment stages before they hit production * separate out real usage metrics from that generated by
developers and tests

### Event schema

Sending custom analytics events to GA is possible from the client-side [via
gtags.js](https://developers.google.com/analytics/devguides/collection/gtagjs/events) or from the server-side [via
Measurement Protocol](https://developers.google.com/analytics/devguides/collection/protocol/v1/devguide). To keep events
consistent, issuers of custom events should follow the following schema.

* `category` - the name of the service issuing the event, e.g. `data-storage-service`, `data-portal`, or `dcp-cli`
* `action` - is a human-readable string representing the function performed, e.g. `download-bundle` or `upload-file`
* `label` - is the identifier of the entity acted on, e.g, bundle UUID or `<organ>-<method>-<data-contributor-last-name>`*

### Frontend instrumentation

For frontend components, we can use the standard GA instrumentation. Steps are included below.

1. Front-end developers create trackers using the standard JavaScript tracking snippet    ([see
   documentation](https://developers.google.com/analytics/devguides/collection/gtagjs/)).
2. Tracking IDs should be chosen based on its deployment environment.
3. At this time, we suggest not customizing the tracking behavior in any way. We expect the default settings will be
   sufficient for our needs, and we can iterate if they are not after we’ve captured early user data.

### Metrics Exporter

The metrics exporter is an application for exporting request/response information from server-side logs to Google
Measurement Protocol API in batch. Its components are as follows.

* A [Kinesis Data Firehose Stream to
  s3](https://docs.aws.amazon.com/firehose/latest/dev/create-destination.html#create-destination-s3) (KDFS)
* A lambda that listens to new s3 events generated by Kinesis Data Firehose, loads the request information and submits
  it   to the Google Measurement Protocol API   [in
  batch](https://developers.google.com/analytics/devguides/collection/protocol/v1/devguide#batch)

### Data storage service instrumentation

The Data Storage Service (DSS) should fire an event whenever data is requested and pushed to users. Steps are included
below.

* Upon each `GET /v1/bundles/:bundle_uuid` and `GET /v1/files/:file_uuid` request, a JSON log line is emitted to
  `stdout` with the key-value pairs `{"log-msg-type": "analytics", "system": "data-storage-service", ...}` and the
  request info
* A subscription for the Metrics Exporter's KDFS is created on DSS's Cloudwatch Logs log group
* The subscription filter limits the stream only to log entries with the key-value pairs outlined above

Approach benefits

* Scalable
* Minimal performance overhead on DSS
* Bulk operations reduce computation, cost, and API rate limiting
* This instrumentation design can be repeated for other data consumer APIs

Approach detractors

* Not as simple as a direct API call
* Not as timely as a direct API call
* Who will manage access to the Google Analytics account with this data?
* How will this data be publicly shared with the wider scientific community, if at all?

### Unresolved questions

* What is the identifier design in event schema?
* Will analysts need extra tools to make sense of event details?

### Drawbacks and Limitations

* Deriving human-readable identifiers will be tricky to balance with keeping systems schema-agnostic.



